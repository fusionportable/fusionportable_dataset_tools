{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize color map (31 colors for 0-30 meters)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "cmap = plt.get_cmap('jet')\n",
    "cmap_colors = (cmap(np.linspace(0, 1, 31)) * 255).astype(np.uint8)\n",
    "\n",
    "def project_points(T_cam_lidar, xyz_points, img, camera, img_path, num_beam):\n",
    "  \"\"\"Project 3D points to image plane with enhanced logic\"\"\"\n",
    "  # Get transformation matrix\n",
    "  R, t = T_cam_lidar[:3, :3], T_cam_lidar[:3, 3]\n",
    "  \n",
    "  # Transform points (vectorized)\n",
    "  xyz_cam = (xyz_points @ R.T) + t\n",
    "  \n",
    "  # Initialize depth image with infinity\n",
    "  depth_img = np.full((camera.height, camera.width), np.inf, dtype=np.float32)\n",
    "  \n",
    "  # Vectorized projection\n",
    "  valid_mask = np.ones(len(xyz_cam), dtype=bool)\n",
    "  uvs = np.zeros((len(xyz_cam), 2))\n",
    "  \n",
    "  for i, p in enumerate(xyz_cam):\n",
    "    success, uv = camera.project(p)\n",
    "    if success:\n",
    "      uvs[i] = uv\n",
    "    valid_mask[i] = success\n",
    "  \n",
    "  # Filter valid points\n",
    "  uvs = uvs[valid_mask].astype(int)\n",
    "  depths = xyz_cam[valid_mask, 2]\n",
    "  colors = cmap_colors[np.clip(depths.astype(int), 0, 30), :]\n",
    "  \n",
    "  # Update depth image (keep closest points)\n",
    "  for (u, v), d in zip(uvs, depths):\n",
    "    if 0 <= u < img.shape[1] and 0 <= v < img.shape[0]:\n",
    "      if d < depth_img[v, u]:\n",
    "        i = int(min(d, 20.0))\n",
    "        color = (int(cmap_colors[i, 0]), int(cmap_colors[i, 1]), int(cmap_colors[i, 2]))        \n",
    "        depth_img[v, u] = d\n",
    "        cv2.circle(img, (u, v), 1, color=color, thickness=-1)\n",
    "\n",
    "  # Save color projection\n",
    "  image_path = Path(img_path.replace('image', f'color_proj_image_{num_beam}'))\n",
    "  image_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "  Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)).save(str(image_path))\n",
    "\n",
    "  # Save depth image (convert to uint16 in millimeters)\n",
    "  depth_img = np.where(np.isfinite(depth_img), depth_img * 1000, 0).astype(np.uint32)\n",
    "  image_path = Path(img_path.replace('image', f'depth_image_{num_beam}'))\n",
    "  image_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "  Image.fromarray(depth_img).save(str(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader.calib.intrinsic_extrinsic_loader import IntrinsicExtrinsicLoader\n",
    "\n",
    "def generate_depthmap_to_kitti360(dataset_path, calib_path, platform, sequence_name, algorithm):\n",
    "  ##### Set up the output data path\n",
    "  kitti360_path = os.path.join(dataset_path, sequence_name, 'kitti360')\n",
    "\n",
    "  ##### Set up the message topic list for different platforms\n",
    "  # Platform\n",
    "  if platform == 'handheld':\n",
    "    from cfg.dataset.cfg_handheld import dataset_sensor_frameid_dict\n",
    "    from cfg.dataset.cfg_handheld import dataset_rostopic_msg_frameid_dict\n",
    "  elif platform == 'ugv':\n",
    "    from cfg.dataset.cfg_ugv import dataset_sensor_frameid_dict\n",
    "    from cfg.dataset.cfg_ugv import dataset_rostopic_msg_frameid_dict\n",
    "  elif platform == 'legged':\n",
    "    from cfg.dataset.cfg_legged import dataset_sensor_frameid_dict\n",
    "    from cfg.dataset.cfg_legged import dataset_rostopic_msg_frameid_dict\n",
    "    if sequence_name == 'legged_grass00':\n",
    "      dataset_sensor_frameid_dict, dataset_rostopic_msg_frameid_dict = \\\n",
    "        filter_sensor('event', dataset_sensor_frameid_dict, dataset_rostopic_msg_frameid_dict)\n",
    "  elif platform =='vehicle':\n",
    "    from cfg.dataset.cfg_vehicle import dataset_sensor_frameid_dict\n",
    "    from cfg.dataset.cfg_vehicle import dataset_rostopic_msg_frameid_dict\n",
    "\n",
    "  ##### Set up the sensor configuration\n",
    "  int_ext_loader = IntrinsicExtrinsicLoader(is_print=False)\n",
    "  int_ext_loader.load_calibration(calib_path=calib_path, sensor_frameid_dict=dataset_sensor_frameid_dict)\n",
    "  print('Finish loading parameters')\n",
    "\n",
    "  # Description: Generate depth map with enhanced projection logic\n",
    "  num_beam_input = 128\n",
    "  # Main processing loop\n",
    "  for frame_id in range(100000):\n",
    "    # Load point cloud data\n",
    "    pcd_path = os.path.join(kitti360_path, 'ouster00_undistort/points/data', f'{frame_id:06d}.pcd')\n",
    "    if not os.path.exists(pcd_path): break\n",
    "    \n",
    "    pcd = o3d.io.read_point_cloud(pcd_path)\n",
    "    if not pcd.has_points(): continue\n",
    "    xyz_points = np.asarray(pcd.points)\n",
    "\n",
    "    for num_beam_output in [128, 64, 32, 16, 8, 4]:\n",
    "      r = np.sqrt(xyz_points[:, 0]**2 + xyz_points[:, 1]**2) + 1e-8\n",
    "      theta = np.arctan(xyz_points[:, 2] / r)\n",
    "      sorted_indices = np.argsort(theta)\n",
    "      sorted_points = xyz_points[sorted_indices]\n",
    "      groups = np.array_split(sorted_points, num_beam_input)\n",
    "      xyz_points = np.concatenate(groups[::int(num_beam_input / num_beam_output)], axis=0)\n",
    "\n",
    "      # Camera processing configuration\n",
    "      camera_configs = [\n",
    "          # Frame cameras\n",
    "          {'type': 'frame', 'suffix': '00', 'cam_spec': ('left', 'right')},\n",
    "          {'type': 'frame', 'suffix': '01', 'cam_spec': ('right', 'right')},\n",
    "          # Event cameras\n",
    "          {'type': 'event', 'suffix': '00', 'cam_spec': ('left',)},\n",
    "          {'type': 'event', 'suffix': '01', 'cam_spec': ('right',)}\n",
    "      ]\n",
    "\n",
    "      for config in camera_configs:\n",
    "          # Generate camera parameters\n",
    "          if config['type'] == 'frame':\n",
    "              cam_side = 'vehicle_' if platform == 'vehicle' else ''\n",
    "              cam_id = f\"{cam_side}frame_{config['cam_spec'][0]}_camera\"\n",
    "              img_folder = f\"{cam_side}frame_cam{config['suffix']}\"\n",
    "          else:\n",
    "              cam_id = f\"event_{config['cam_spec'][0]}_camera\"\n",
    "              img_folder = f\"event_cam{config['suffix']}\"\n",
    "\n",
    "          # Build image path\n",
    "          img_path = os.path.join(\n",
    "              kitti360_path,\n",
    "              f'{img_folder}/image/data',\n",
    "              f'{frame_id:06d}.png'\n",
    "          )\n",
    "\n",
    "          # Load and validate image\n",
    "          img = cv2.imread(img_path)\n",
    "          if img is None:\n",
    "              print(f\"Missing image: {img_path}\")\n",
    "              continue\n",
    "\n",
    "          # Get camera parameters and transformation\n",
    "          camera = int_ext_loader.sensor_collection[cam_id]\n",
    "          T_cam_lidar = int_ext_loader.tf_graph.get_relative_transform(camera.frame_id, 'body_imu')\n",
    "          project_points(T_cam_lidar, xyz_points, img.copy(), camera, img_path, num_beam_output)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from PIL import Image\n",
    "from cfg.dataset.cfg_sequence import dataset_sequence_calib_used_dict\n",
    "\n",
    "dataset_path = '/Rocket_ssd/dataset/data_FusionPortable/sensor_data'\n",
    "algorithms = ['fastlio2']\n",
    "\n",
    "for sequence_name, values in dataset_sequence_calib_used_dict.items():\n",
    "  for algorithm in algorithms:\n",
    "    platform, calib_folder, used = values[0], values[1], values[2]\n",
    "    if used:\n",
    "      calib_path = os.path.join(dataset_path, '../calibration_files', calib_folder, 'calib')\n",
    "      print('Start processing platform: {} sequence: {} with algorithm_result: {}'.format(platform, sequence_name, algorithm))\n",
    "      \n",
    "      generate_depthmap_to_kitti360(dataset_path, calib_path, platform, sequence_name, algorithm)\n",
    "      print('Finish generating depth images')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp_dataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
